"""√âvaluation du mod√®le fine-tunn√©"""
import torch
from transformers import (
    Trainer, 
    DataCollatorForLanguageModeling, 
    AutoModelForCausalLM, 
    AutoTokenizer, 
    BitsAndBytesConfig
)
from datasets import load_dataset
import numpy as np
from tqdm import tqdm
import json
from datetime import datetime
from typing import List, Dict
from src.model import load_finetuned_model
from src.dataset import load_prepare_dataset

# ============================================
# √âVALUATION PRINCIPALE
# ============================================

def evaluate_model(
    adapter_path: str = "/kaggle/working/results/checkpoint-75",
    base_model_name: str = "mistralai/Mistral-7B-Instruct-v0.1",
    data_dir: str = "/kaggle/working/data",
    device: str = "cuda",
    batch_size: int = 4,
    sample_size: int = 10,
    save_results: bool = True,
) -> Dict:
    """
    √âvalue le mod√®le avec m√©triques compl√®tes et sauvegarde
    """
    
    print("=" * 70)
    print("√âVALUATION DU MOD√àLE FINE-TUN√â")
    print("=" * 70) 
    
    # ============================================
    # CHARGEMENT
    # ============================================
    print(f"\nChargement du mod√®le...") 
    try:
        model, tokenizer = load_finetuned_model(
            base_model_name=base_model_name,
            adapter_path=adapter_path,
            device=device
        )
        print("Mod√®le charg√©")
    except Exception as e:
        print(f"Erreur de chargement: {e}")
        return None
    
    print("\nChargement du dataset de test...")
    try:
        datasets = load_prepare_dataset(
            data_dir=data_dir,
            tokenizer=tokenizer,
            max_length=512,
        )
        test_dataset = datasets["test_dataset"]
        print(f"Dataset: {len(test_dataset)} exemples")
    except Exception as e:
        print(f"Erreur de chargement dataset: {e}")
        return None

    # ============================================
    # √âVALUATION QUANTITATIVE
    # ============================================
    print("\n" + "=" * 70)
    print("√âVALUATION QUANTITATIVE")
    print("=" * 70)
    
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
    )

    trainer = Trainer(
        model=model,
        data_collator=data_collator,
        eval_dataset=test_dataset,
    )

    print("\nCalcul de la loss et perplexit√©...")
    try:
        test_results = trainer.evaluate()
        eval_loss = test_results["eval_loss"]
        perplexity = torch.exp(torch.tensor(eval_loss)).item()
        test_results["perplexity"] = perplexity

        print("\nR√©sultats quantitatifs:")
        print(f"   ‚Ä¢ Loss: {eval_loss:.4f}")
        print(f"   ‚Ä¢ Perplexit√©: {perplexity:.4f}")
            
    except Exception as e:
        print(f"Erreur lors de l'√©valuation: {e}")
        test_results = {"eval_loss": None, "perplexity": None}

    # ============================================
    # SAUVEGARDE
    # ============================================
    if save_results:
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            results_dict = {
                "timestamp": timestamp,
                "model": base_model_name,
                "adapter_path": adapter_path, 
                "checkpoint": "checkpoint-75",
                "quantitative_metrics": {
                    "eval_loss": test_results.get("eval_loss"),
                    "perplexity": test_results.get("perplexity"),
                },
            }
                       
            output_file = f"/kaggle/working/evaluation_checkpoint75_{timestamp}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results_dict, f, indent=2, ensure_ascii=False)
            
            print(f"\nR√©sultats sauvegard√©s: {output_file}")
        except Exception as e:
            print(f"Erreur lors de la sauvegarde: {e}")
    
    print("\n" + "=" * 70)
    print("√âVALUATION TERMIN√âE")
    print("=" * 70)
    
    return {
        "quantitative": test_results,
    }


# ============================================
# COMPARAISON BASELINE
# ============================================

def compare_with_baseline(
    adapter_path: str = "/kaggle/working/results/checkpoint-75", 
    base_model_name: str = "mistralai/Mistral-7B-Instruct-v0.1",
    data_dir: str = "/kaggle/working/data",
    device: str = "cuda",
    num_examples: int = 3,
) -> None:
    """
    Compare avec le mod√®le de base (version optimis√©e)
    """
    
    print("\n" + "=" * 70)
    print("COMPARAISON MOD√àLE DE BASE VS FINE-TUN√â")
    print("=" * 70)
    
    # Charger les exemples
    try:
        dataset = load_dataset("json", data_files={"test": f"{data_dir}/test.json"})["test"]
    except Exception as e:
        print(f"Erreur de chargement: {e}")
        return
    
    # Pr√©parer la config
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )
    
    # Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(base_model_name)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"
    
    # Charger les mod√®les
    print("\nChargement du mod√®le de base...")
    try:
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
        )
        print("Mod√®le de base charg√©")
    except Exception as e:
        print(f"Erreur: {e}")
        return
    
    print("Chargement du mod√®le fine-tun√©...")
    try:
        finetuned_model, _ = load_finetuned_model(
            base_model_name=base_model_name,
            adapter_path=adapter_path,
            device=device
        )
        print("Mod√®le fine-tun√© charg√©")
    except Exception as e:
        print(f"Erreur: {e}")
        return
    
    # Comparer sur plusieurs exemples
    for idx in range(min(num_examples, len(dataset))):
        example = dataset[idx]
        test_question = example["messages"][0]["content"]
        expected_answer = example["messages"][1]["content"]
        
        print(f"\n{'‚îÄ' * 70}")
        print(f"EXEMPLE {idx + 1}/{num_examples}")
        print(f"{'‚îÄ' * 70}")
        print(f"\nQuestion:\n{test_question}")
        print(f"\nR√©ponse attendue:\n{expected_answer}")
        
        prompt = f"### Question:\n{test_question}\n\n### R√©ponse:\n"
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        # Mod√®le de BASE
        print("\nüîµ Mod√®le de BASE:")
        try:
            with torch.no_grad():
                outputs = base_model.generate(
                    **inputs,
                    max_new_tokens=512,
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                    repetition_penalty=1.1,
                )
            
            base_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            if "### R√©ponse:" in base_response:
                base_response = base_response.split("### R√©ponse:")[-1].strip()
            
            print(base_response)
        except Exception as e:
            print(f"Erreur: {e}")
        
        # Mod√®le FINE-TUN√â
        print("\nüü¢ Mod√®le FINE-TUN√â:")  
        try:
            with torch.no_grad():
                outputs = finetuned_model.generate(
                    **inputs,
                    max_new_tokens=512,
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                    repetition_penalty=1.1,
                )
            
            finetuned_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            if "### R√©ponse:" in finetuned_response:
                finetuned_response = finetuned_response.split("### R√©ponse:")[-1].strip()
            
            print(finetuned_response)
        except Exception as e:
            print(f"Erreur: {e}")
    
    print("\n" + "=" * 70)


# ============================================
# MAIN
# ============================================

if __name__ == "__main__":
    # √âvaluation compl√®te
    print("üöÄ D√©marrage de l'√©valuation compl√®te (CHECKPOINT-75)...\n")  # ‚úÖ MODIFI√â
    
    results = evaluate_model(
        adapter_path="/kaggle/working/results/checkpoint-75",  # ‚úÖ MODIFI√â
        base_model_name="mistralai/Mistral-7B-Instruct-v0.1",
        data_dir="/kaggle/working/data",
        sample_size=10,
        save_results=True,
    )
    
    # Comparaison avec baseline
    if results is not None:
        compare_with_baseline(
            adapter_path="/kaggle/working/results/checkpoint-75",  # ‚úÖ MODIFI√â
            base_model_name="mistralai/Mistral-7B-Instruct-v0.1",
            data_dir="/kaggle/working/data",
            num_examples=3,
        )